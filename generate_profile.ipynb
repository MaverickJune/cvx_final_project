{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install all the necessary packages for inference and fine-tuning (may not use all of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nxclab/anaconda3/envs/vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from textwrap import dedent\n",
    "from typing import Dict, List\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from colored import Back, Fore, Style\n",
    "from datasets import Dataset, load_dataset\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "from trl import DataCollatorForCompletionOnlyLM, SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for model inference profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "access_token = \"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\"\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(SEED)\n",
    "PAD_TOKEN = \"<|pad|>\"\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloding the toknizer...\n",
      "Done\n",
      "Downloding the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloding the toknizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, token=access_token)\n",
    "tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "tokenizer.padding_side = \"left\"\n",
    "print(\"Done\")\n",
    "\n",
    "print(\"Downloding the model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map = \"auto\",\n",
    "    token=access_token\n",
    ")\n",
    "print(\"Done\")\n",
    "\n",
    "# check the mapping of the model\n",
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a knowledgeable, efficient, and direct AI assistant. Provide concise answers, focusing on the key information needed. Offer suggestions tactfully when appropriate to improve outcomes. Engage in productive collaboration with the user.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "length: 84\n"
     ]
    }
   ],
   "source": [
    "# Now, only consider simple examples (only questions)\n",
    "# system prompt from : https://www.reddit.com/r/LocalLLaMA/comments/1cry85p/lmstudio_better_system_prompt_for_llama_3_8b_and/\n",
    "# llama 3 formatting : https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/\n",
    "\n",
    "def format_example(input: dict, tokenizer: AutoTokenizer):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "    {input[\"question\"]}\n",
    "        \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a knowledgeable, efficient, and direct AI assistant. Provide concise answers, focusing on the key information needed. Offer suggestions tactfully when appropriate to improve outcomes. Engage in productive collaboration with the user.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"\"\n",
    "        }\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False).rstrip(\"<|eot_id|>\").strip()\n",
    "\n",
    "def count_tokens(input: str, tokenizer: AutoTokenizer) -> int:\n",
    "    return len(\n",
    "        tokenizer(\n",
    "            input,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=False\n",
    "        )['input_ids']\n",
    "    )\n",
    "\n",
    "# testcode\n",
    "input_example = {\n",
    "    \"question\": \"What is the capital of France?\"\n",
    "}\n",
    "formatted_input = format_example(input_example, tokenizer)\n",
    "print(formatted_input)\n",
    "print(f'length: {count_tokens(formatted_input, tokenizer)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare custom dataset(dummy) for speed measurement\n",
    "def prepare_dataset_for_speed_eval(input: dict, batch_size: int, tokenizer: AutoTokenizer):\n",
    "    input_batch = []\n",
    "    for _ in range(batch_size):\n",
    "        formatted_input = format_example(input, tokenizer)\n",
    "        input_batch.append(formatted_input)\n",
    "    \n",
    "    return tokenizer(input_batch, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 91])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "input_example = {\n",
    "    \"question\": \"Who is the richest person in the world? Is it Elon Musk?\"\n",
    "}\n",
    "dataset = prepare_dataset_for_speed_eval(input_example, BATCH_SIZE, tokenizer).to(\"cuda\")\n",
    "print(dataset['input_ids'].shape)\n",
    "print(dataset['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile prefill/decoding stage of the LLM (Demo version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up...\n",
      "Warming up done \n",
      "\n",
      "Profiling step 1\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Profiling step 2\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Profiling step 3\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Profiling step 4\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Profiling step 5\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Profiling step 6\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Profiling step 7\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Profiling step 8\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Profiling step 9\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Profiling step 10\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Profiling step 11\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Profiling step 12\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Profiling step 13\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Profiling step 14\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Profiling step 15\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Profiling step 16\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Profiling step 17\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Profiling step 18\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Profiling step 19\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Profiling step 20\n",
      "Start measuring prefill latency...\n",
      "Prefill Measurement done\n",
      "Start measuring decoding latency...\n",
      "Decoding Measurement done\n",
      "Result saved, profiling done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "WARMUP_STEPS = 50\n",
    "PREFILL_STEPS = 20\n",
    "DECODING_STEPS = 20\n",
    "MAX_NEW_TOKENS = 50\n",
    "\n",
    "prefill_latencies_board = []\n",
    "decoding_latencies_board = []\n",
    "\n",
    "questions = [\n",
    "    {\"question\": \"Do you like music?\"},\n",
    "    {\"question\": \"What time do you wake up?\"},\n",
    "    {\"question\": \"Which season do you like most?\"},\n",
    "    {\"question\": \"What do you usually eat for breakfast?\"},\n",
    "    {\"question\": \"Can you name three things you enjoy about your job or studies?\"},\n",
    "    {\"question\": \"If you had a superpower, what would it be, and how would you use it?\"},\n",
    "    {\"question\": \"How do you spend your weekends, and what makes them enjoyable?\"},\n",
    "    {\"question\": \"Can you share a memorable experience from your childhood in a few sentences?\"},\n",
    "    {\"question\": \"If you could change one thing about the world, what would it be and why do you think it’s important?\"},\n",
    "    {\"question\": \"Imagine you have unlimited resources for a year. What would you do, and how would it impact others?\"},\n",
    "    {\"question\": \"If you were tasked with organizing a community event to bring people together, what type of event would you plan, and how would you ensure its success?\"},\n",
    "    {\"question\": \"Suppose you were given the opportunity to learn any skill instantly. Which skill would you choose, and how do you think it would improve your life?\"},\n",
    "    {\"question\": \"Imagine you could meet a famous historical figure from the past. Who would it be, what questions would you ask them, and why would you choose this person?\"},\n",
    "    {\"question\": \"If you were to write a book about your life, what would the title be, and what key lessons or experiences would you include in the story?\"},\n",
    "    {\"question\": \"Suppose you had to design a plan to make your community more environmentally friendly. What specific changes or initiatives would you propose, and how would you encourage others to participate?\"},\n",
    "    {\"question\": \"If you could live in a different era or time period, which one would you choose, and what aspects of life during that time appeal to you the most?\"},\n",
    "    {\"question\": \"Imagine you are offered a chance to create a new invention that could solve a pressing global problem. What would your invention do, and how would it impact society?\"},\n",
    "    {\"question\": \"Consider a situation where you are asked to give a speech to inspire young people to follow their dreams. What message would you share, and what examples from your own life would you use to motivate them?\"},\n",
    "    {\"question\": \"If you were in charge of a large project that required managing a diverse team of people, how would you foster collaboration, resolve conflicts, and ensure the project's success?\"},\n",
    "    {\"question\": \"Picture a scenario where you have unlimited resources to improve education globally. What changes would you implement, how would you address inequalities, and what long-term outcomes would you hope to achieve from your initiatives?\"},\n",
    "]\n",
    "\n",
    "\n",
    "datasets = [prepare_dataset_for_speed_eval(question, BATCH_SIZE, tokenizer).to(\"cuda\") for question in questions]\n",
    "dataset_lengths_in_tokens = [count_tokens(format_example(question, tokenizer), tokenizer) for question in questions]\n",
    "\n",
    "# pad_token_matching\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Profiling prefill\n",
    "with torch.inference_mode():\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        prefill_latencies = []\n",
    "        prefill_decoding_latencies = []\n",
    "        \n",
    "        # warmup\n",
    "        if i == 0:\n",
    "            print(\"Warming up...\")\n",
    "            torch.cuda.synchronize()\n",
    "            for _ in range(WARMUP_STEPS):\n",
    "                model.generate(**dataset, max_new_tokens = 1)\n",
    "            torch.cuda.synchronize()\n",
    "            print(\"Warming up done \\n\")\n",
    "        \n",
    "        print(f'Profiling step {i+1}')\n",
    "        # actual measurement(prefill)\n",
    "        print(\"Start measuring prefill latency...\")\n",
    "        for _ in range(PREFILL_STEPS):\n",
    "            torch.cuda.synchronize()\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            start.record()\n",
    "            outputs = model.generate(**dataset, max_new_tokens = 1)\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            prefill_latencies.append(start.elapsed_time(end))\n",
    "        print(\"Prefill Measurement done\")\n",
    "        \n",
    "        # actual measurement(decoding)\n",
    "        print(\"Start measuring decoding latency...\")\n",
    "        for _ in range(DECODING_STEPS):\n",
    "            torch.cuda.synchronize()\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            start.record()\n",
    "            outputs = model.generate(**dataset, max_length=MAX_NEW_TOKENS+dataset_lengths_in_tokens[i])\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            prefill_decoding_latencies.append(start.elapsed_time(end))\n",
    "        print(\"Decoding Measurement done\")\n",
    "        \n",
    "        # Record the latencies\n",
    "        decoding_latencies_per_token = [(a - b)/MAX_NEW_TOKENS for a, b in zip(prefill_decoding_latencies, prefill_latencies)]\n",
    "        avg_prefil = sum(prefill_latencies) / PREFILL_STEPS\n",
    "        avg_decode = sum(prefill_decoding_latencies) / (DECODING_STEPS * MAX_NEW_TOKENS)\n",
    "        prefill_latencies_board.append(avg_prefil)\n",
    "        decoding_latencies_board.append(avg_decode)\n",
    "        \n",
    "# Save the results\n",
    "results = {\n",
    "    \"prefill_latencies\": prefill_latencies_board,\n",
    "    \"decoding_latencies\": decoding_latencies_board,\n",
    "    \"dataset_lengths_in_tokens\": dataset_lengths_in_tokens\n",
    "}\n",
    "with open(\"profiling_results.json\", \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "    \n",
    "print(\"Result saved, profiling done\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
